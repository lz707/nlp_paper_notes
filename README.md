# NLP Paper Notes
Notes taken while reading NLP papers

## White papers
I have authored several white papers regarding the application of Deep Learning and Natual Language Processing (NLP) in quantitative investment and finance. 
- [Utilizing Natural Language Processing to Bolster Active Management](./NLP_white_paper_part0.pdf)
- [The next-generation alpha model: Introducing natural language processing](./NLP_white_paper_part1.pdf)
- [The next-generation alpha model: seeding intelligence with the ground truth](./NLP_white_paper_part2.pdf)

## Transformer
- NMT Attention: [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) [[Notes](nmt_attention.md)] <kbd>ICLR 2015</kbd>
- Local Attention: [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) [[Notes](local_attention.md)] <kbd>EMNLP 2015</kbd>
- Transformer: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) [[Notes](transformer.md)] <kbd>NeurIPS 2017</kbd>
- TransformerXL: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) [[Notes](transformerxl.md)] <kbd>ACL 2019</kbd>
- BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) [[Notes](bert.md)] <kbd>NAACL 2019</kbd>
- XLNet: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) [[Notes](xlnet.md)] <kbd>NeurIPS 2019</kbd>
- RoBERTa: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)[[Notes](roberta.md)]
- ALBERT: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)[[Notes](albert.md)]
- ELECTRA: [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB)[[Notes](electra.md)] <kbd>ICLR 2020</kbd>

## Document Classification
- SIF: [A Simple but Tough-to-Beat Baseline for Sentence Embeddings](https://openreview.net/forum?id=SyK00v5xx) [[Notes](sif.md)] <kbd>ICLR 2017</kbd>
- Fasttext: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759) [[Notes](fasttext.md)] <kbd>EACL 2017</kbd>
- Sentence CNN: [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) [[Notes](cnn.md)] <kbd>EMNLP 2014</kbd>
- Character CNN: [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626) [[Notes](char_cnn.md)] <kbd>NeurIPS 2015</kbd>
- Wide Convolution: [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/abs/1404.2188) [[Notes](wide_cnn.md)] <kbd>ACL 2014</kbd>


## Sequence to Sequence
- LSTM: [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) [[Notes](lstm.md)] <kbd>NeurIPS 2014</kbd>
- CNN: [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) [[Notes](cnn.md)] <kbd>ICML 2017</kbd>
- Back-translation: [Phrase-based & neural unsupervised machine translation](https://arxiv.org/abs/1804.07755) [[Notes](back_translation.md)] <kbd>EMNLP 2018</kbd>
- Google MT: [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144) [[Notes](googlemt.md)] 


## Question Answering/Reading Comprehension
- QANet: [QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension](https://arxiv.org/abs/1804.09541) [[Notes](qanet.md)] <kbd>ICLR 2018</kbd>
- Bi-Attention: [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603) [[Notes](bi_attention.md)] <kbd>ICLR 2017</kbd>

## Generative
- GAN: [Adversarial Learning for Neural Dialogue Generation](https://arxiv.org/abs/1701.06547) [[Notes](gan.md)] <kbd>ACL 2017</kbd>
- SeqGAN: [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473) [[Notes](seqgan.md)] <kbd>AAAI 2017</kbd>

## Information Extraction
- Relation Extraction using noisy data: [Neural Relation Extraction with Selective Attention over Instances](https://www.aclweb.org/anthology/P16-1200.pdf) [[Notes](noisy_re.md)] <kbd>ACL 2016</kbd>
- LSTM-CNNs-CRF: [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](https://arxiv.org/abs/1603.01354) [[Notes](lstm_cnns_crf.md)] <kbd>ACL 2016</kbd>


## Knowledge Graph
- GCN: [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103) [[Notes](gcn.md)] <kbd>ICLR 2017</kbd>


## Practical Tricks
- UDA: [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848) [[Notes](uda.md)]

