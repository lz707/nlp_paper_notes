# [Neural Relation Extraction with Selective Attention over Instances](https://www.aclweb.org/anthology/P16-1200.pdf)
Written in 2016

tl;dr: The authors proposed a sentence-level attention-based model for relation extraction which could automatically alleviate the issue of wrong labels generated by distant supervision.

#### Overall impression
The paper introduced a practical structure that utilizes the automatically labeled relation data. Its attention mechanism for reduce label noise is inspirational to other research that faces a similar problem. 

#### Key ideas
Relation extraction is one of the most important tasks in NLP. Many efforts have been invested in relation extraction, especially in supervised relation extraction. Most of these methods need a great deal of annotated data, which is time consuming and labor intensive. 

In recent years, various large-scale knowledge bases (KBs) such as Freebase, DBpedia and YAGO have been built. These KBs mostly compose of relational facts with triple format, e.g., (Microsoft, founder, Bill Gates).

In order to generate automatically annotated data, (Mintz et al., 2009) aligns plain text with Freebase by distant supervision. They assume that if two entities have a relation in KBs, then all sentences that contain these two entities will express this relation. For example, distant supervision will regard all sentences that contain “Microsoft” and “Bill Gates” as active instances for relation “founder”. Although this method is effective, it is not always hold for every sentence, therefore, generated a lot of wrong labels. 

To alleviate this issue, the authors proposed a sentence-level attention-based model for relation extraction and employed convolutional neural networks to embed the semantics of sentences. Then they built sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. 

#### Technical details
Model structure:
- Sentence Embedding

![](https://user-images.githubusercontent.com/10768193/56016251-97541d00-5d36-11e9-9729-717a9313049f.png)

- Selective Attention: after get all the sentence embedding, calculate attention between input sentence and predict relation. If the two matches at a higher level, the attention weight would be higher. This way the wrong labelling, where the relation is not mentioned at all in the input sentence, will get a lower weight. 

#### Notes
